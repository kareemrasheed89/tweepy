{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemrasheed89/tweepy/blob/master/twitter_sentiment_bertModel_Copy2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing necessary libraries\n",
        "!pip install config --upgrade"
      ],
      "metadata": {
        "id": "JCcpLatZSXnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cf9f01-b89c-4737-c3c0-411868ece269"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting config\n",
            "  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: config\n",
            "Successfully installed config-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy --upgrade"
      ],
      "metadata": {
        "id": "qIEWr3CSSum-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce778e7-2a2c-4425-c45f-be58f1b30827"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (3.2.0)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
            "Installing collected packages: requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed requests-2.28.1 tweepy-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2cepL7hXoDv",
        "outputId": "5f79e04c-72b6-4dc3-e12b-358211c01b62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UzrYLoAX3In",
        "outputId": "b1a797a8-7849-4fd7-eb16-7b4f8d0923cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.8.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "\u001b[K     |████████████████████████████████| 435 kB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from wordcloud) (3.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
            "Installing collected packages: wordcloud\n",
            "  Attempting uninstall: wordcloud\n",
            "    Found existing installation: wordcloud 1.5.0\n",
            "    Uninstalling wordcloud-1.5.0:\n",
            "      Successfully uninstalled wordcloud-1.5.0\n",
            "Successfully installed wordcloud-1.8.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U_rTSSvX9wq",
        "outputId": "a13e2cc6-0bc5-41d5-a13d-5cf0edb05f0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "081ekEpTRbgK"
      },
      "outputs": [],
      "source": [
        "# importing `config.py` to access its variables and other needed dependencies\n",
        "import config\n",
        "from tweepy import Client\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from wordcloud import WordCloud\n",
        "%matplotlib inline\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E1vPS3kfRbgQ"
      },
      "outputs": [],
      "source": [
        "#getting recent tweets data\n",
        "def getTweets():\n",
        "    #asking for the search term and the desired number of tweets\n",
        "    global keyword \n",
        "    keyword = input('insert search term : ')\n",
        "    #what is the research query e.g. \n",
        "    query = f'{keyword} -is:retweet lang:en'# place_country:GB' #no retweets, language must be English and Country UK.\n",
        "    \n",
        "    #input the number tweets required.\n",
        "    num_tweets = int(input('how many tweets do you want? '))\n",
        "    #connecting to the twitter API using clent and the bearer_token credentials from config.py\n",
        "    #client = Client(config.bearer_token)\n",
        "    \n",
        "    client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAGBqVgEAAAAAUvY9oOpJTR8DUWV4sjAQVYj06BA%3Dy0A2A7CX5E9XCqwaYi8pFdkMqk2GqLnBv8Phd2Y5XTv6hJ5ZSx')\n",
        "    \n",
        "    # Replace with time period of your choice\n",
        "    start_time = '2021-01-01T00:00:00Z'\n",
        "\n",
        "    # Replace with time period of your choice\n",
        "    end_time = '2021-03-31T00:00:00Z'\n",
        "\n",
        "    #using tweepy paginator to get over 100 last tweets from twitter api\n",
        "    tweets = []\n",
        "    for tweet in tweepy.Paginator(client.search_all_tweets,\n",
        "                                    query = query,                             \n",
        "                                    tweet_fields = ['id','created_at', 'text'],#, 'geo'], \n",
        "                                    start_time = start_time, end_time = end_time,\n",
        "                                    max_results = 100).flatten(limit=num_tweets):\n",
        "        time.sleep(1)\n",
        "    \n",
        "        tweets.append(tweet)\n",
        "    if len(tweets) <= 20:\n",
        "        print(\"The search for \" + keyword + \" returned an insignificant number of tweets; script will terminate\")\n",
        "        sys.exit(\"The search for \" + keyword + \" returned an insignificant number of tweets; script will terminate\")\n",
        "    else:\n",
        "        return tweets\n",
        "\n",
        "\n",
        "#function to clean the tweets and load them into a DataFrame\n",
        "def tweetsETL(tweets):\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    \n",
        "    #regex function to clean the tweet text from haashtags, mentions and links\n",
        "    def cleanTweets(text):\n",
        "        clean_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text.lower()).split())\n",
        "        #clean_text = [x for x in clean_text if x in unwanted_words]\n",
        "        return clean_text\n",
        "    \n",
        "    #function to unpack the tweets list into a dataframe\n",
        "    for tweet in tweets:\n",
        "            print(tweet)\n",
        "            result.append({'id': tweet.id,\n",
        "                           'text': tweet.text,\n",
        "                           'clean_tweet' : cleanTweets(tweet.text),\n",
        "                           'created_at': tweet.created_at,\n",
        "                           #'lang' : tweet.lang,\n",
        "                           #'geo' : tweet.geo['place_id']\n",
        "                           #'source':tweet.source,\n",
        "                           #'retweets': tweet.public_metrics['retweet_count'],\n",
        "                           #'replies': tweet.public_metrics['reply_count'],\n",
        "                           #'likes': tweet.public_metrics['like_count'],\n",
        "                           #'quote_count': tweet.public_metrics['quote_count']\n",
        "                      })\n",
        "    #result = [x for x in result if x not in unwanted_words]\n",
        "    df = pd.DataFrame(result)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ucx1SuOIRbgS"
      },
      "outputs": [],
      "source": [
        "#using a transformers model \"bert\" to perform the sentiment analysis on the clean_tweets column.\n",
        "def sentimentAnalysis(df):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "    res = df['clean_tweet'].apply(lambda x: classifier(x[:512]))\n",
        "    #print(res)\n",
        "    return res\n",
        "\n",
        "## function to add the list resulting from the analysis to the original dataframe as score, sentiment and stars\n",
        "#the sentiment is either negative, positive or neutral, and the number of stars go from 1 to 5\n",
        "#1 being the most negative sentiment and 5 being the most positive\n",
        "def sentimentToDf(df,res):\n",
        "    \n",
        "    \n",
        "    tweets_stars = []\n",
        "    tweets_scores = []\n",
        "    tweets_sentiment = []\n",
        "    #looping over the list of result to unpack it into the original tweets dataframe\n",
        "    for i in range(res.size):\n",
        "        tweets_stars.append(int(float(res[i][0]['label'].split()[0])))\n",
        "        tweets_scores.append(res[i][0]['score'])\n",
        "        if res[i][0]['label'] == '4 stars' or res[i][0]['label'] == '5 stars':\n",
        "            tweets_sentiment.append('positive')\n",
        "        elif res[i][0]['label'] == '1 star' or res[i][0]['label'] == '2 stars':\n",
        "            tweets_sentiment.append('negative')\n",
        "        else :\n",
        "            tweets_sentiment.append('neutral')\n",
        "    df['scores'] = tweets_scores\n",
        "    df['sentiment'] = tweets_sentiment  \n",
        "    df['stars'] = tweets_stars\n",
        "    \n",
        "    #----------------------------------4/2/2022--------------------------------------------------------------#\n",
        "    #list frequent keywords to be filtered from the wordcloud\n",
        "    unwanted_words = ['vaccine', 'vaccines', 'vaccination', 'covid', 'covid-19', 'covid19', 'people', 'amp']\n",
        "    \n",
        "    #remove most frequent keywords and stopwords\n",
        "    filtered_tweets = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for rmv in df['clean_tweet']:\n",
        "        remove_keywords = [x for x in rmv.split() if x not in unwanted_words]\n",
        "        remove_keywords = [x for x in remove_keywords if x not in stop_words]\n",
        "        #print(remove_keywords)\n",
        "        filtered_tweets.append(str(remove_keywords).replace(\"'\",\"\"))\n",
        "    \n",
        "    #update clean_tweets column in the dataframe\n",
        "    df['clean_tweet'] = filtered_tweets\n",
        "    #-------------------------------------------------------------------------------------------------------#\n",
        "    \n",
        "    #write to csv file\n",
        "    df.to_csv('fileok.csv')\n",
        "    return df\n",
        "\n",
        "#fucntion to Create the wordclouds using data frpom a column of a dataframe\n",
        "def creatWordCloud(df,clm_name):\n",
        "    text = \" \".join(line for line in df[clm_name])\n",
        "    # Create the wordcloud object\n",
        "    wordcloud = WordCloud(width=980, height=580, margin=0,collocations = False, background_color = 'white').generate(text)\n",
        "    # Display the generated image:\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.margins(x=0, y=0)\n",
        "    plt.show()\n",
        "    return plt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CC_JGc3MRbgU"
      },
      "outputs": [],
      "source": [
        "#creating a function to show the result of the sentiment analysis from the final df   \n",
        "def showReport(df):     \n",
        "    print(f'* the tweets show that the sentiment around \"{keyword}\" is mainly {df.groupby(by=\"sentiment\").id.count().sort_values(ascending=False).index[0] }')\n",
        "    print(f'* this is how the overall sentiment and stars ratings breakdown on the {len(df)} total records we recovered : ')\n",
        "    print(df.groupby([\"stars\"]).count()['id'])\n",
        "    \n",
        "     # Build the percentage of star count reviews by category pie chart.\n",
        "    star_perc = 100 * df.groupby([\"stars\"]).count()['id'] / len(df)\n",
        "    plt.pie(star_perc,\n",
        "            labels=[\"1 star\", \"2 stars\", \"3 stars\", '4 stars', '5 stars'],\n",
        "            colors=[\"red\", \"orange\", \"gold\", 'turquoise', 'green'],\n",
        "            explode=[0.05, 0.05, 0.05, 0.05, 0.05],\n",
        "            autopct='%1.1f%%',\n",
        "            shadow=True, startangle=150),\n",
        "    plt.title(\"percentage of Total tweets by star ratings\")\n",
        "    # Show Figure\n",
        "    plt.show()\n",
        "     # Build the sentiment reviews by category pie chart.\n",
        "    sent_perc = 100 * df.groupby([\"sentiment\"]).count()['id'] / len(df)   \n",
        "    plt.pie(sent_perc,\n",
        "            labels=[\"Negative\", \"Neutral\", \"Positive\",],\n",
        "            colors=[\"red\", \"gold\", 'green'],\n",
        "            explode=[0.05, 0.05, 0.05],\n",
        "            autopct='%1.1f%%',\n",
        "            shadow=True, startangle=150),\n",
        "    plt.title(\"percentage of total tweets by sentiment \")\n",
        "    # Show Figure\n",
        "    plt.show()\n",
        "    \n",
        "#function to creat word clouds for each tweet sentiment\n",
        "def sentimentWordcloud(df):\n",
        "    print(\"We generate Wordclouds for each sentiment to see the words that appear most often for each one :\")\n",
        "    print(\"_________________________________________________________________________________________________\")\n",
        "    print('Wordcloud for negative sentiment tweets : ')\n",
        "    creatWordCloud(df.query('sentiment == \"negative\"'),\"clean_tweet\")\n",
        "    print('Wordcloud for neutral sentiment tweets : ')\n",
        "    creatWordCloud(df.query('sentiment == \"neutral\"'),\"clean_tweet\")\n",
        "    print('Wordcloud for positive sentiment tweets : ')\n",
        "    creatWordCloud(df.query('sentiment == \"positive\"'),\"clean_tweet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zzhfOm8gRbgV"
      },
      "outputs": [],
      "source": [
        "# creating call functions\n",
        "\n",
        "def tweetTodf():\n",
        "    print (\"this is a simple twitter sentiment analysis bot, please follow the instuction to know twitter's last thoughts. \\n- the tweets collected are the last specified number of tweets about a keyword of your choice\")\n",
        "    print ('connection to twitter IPA')\n",
        "    df = tweetsETL(getTweets())\n",
        "    print ('retrieving tweets -- ')\n",
        "    return df\n",
        "\n",
        "def sentimentTodf(df):\n",
        "    print('-----------------------------------------')\n",
        "    print('sentiment analysis in progress.')\n",
        "    print('this might take a minute ...')\n",
        "    final_df = sentimentToDf(df,sentimentAnalysis(df))\n",
        "    print('-----------------------------------------')\n",
        "    return final_df\n",
        "\n",
        "def finalReport(final_df):\n",
        "    print('creating report ...')\n",
        "    print(f'the report represents the sentiment around \"{keyword}\", stars represent the sentiment of a tweet \\n from 1 being most negative to 5 being most positive ...')\n",
        "    print(\"_________________________________________________________________________________________________\\n\")\n",
        "    showReport(final_df)\n",
        "    sentimentWordcloud(final_df)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0ImbDHaRbgW",
        "outputId": "b0d9faaa-7e88-4d6a-95f2-84d713cd4011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this is a simple twitter sentiment analysis bot, please follow the instuction to know twitter's last thoughts. \n",
            "- the tweets collected are the last specified number of tweets about a keyword of your choice\n",
            "connection to twitter IPA\n",
            "insert search term : high self-esteem\n",
            "how many tweets do you want? 10000\n"
          ]
        }
      ],
      "source": [
        "finalReport(sentimentTodf(tweetTodf()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUFc3R7LRbgX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DT1qW_-RbgY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs-KbKk8RbgZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "twitter sentiment_bertModel-Copy2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}